{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/thom/Desktop/CIPM/FruitFlyPheno\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import timedelta\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "\n",
    "# -----------\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "# from utils.inputs import *\n",
    "# from utils.outputs import *\n",
    "\n",
    "from utils import (\n",
    "    validate_inputs,\n",
    "    load_species_params,\n",
    "    all_historical_model_run,\n",
    "    prediction_model_run,\n",
    "    fflies_output_class,\n",
    "    PredictionNeededError,\n",
    ")\n",
    "\n",
    "# from utils.outputs import fflies_output_class\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "data_path = \"data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Dict, Tuple, Union, List\n",
    "import xarray as xr\n",
    "from utils.degree_day_equations import single_sine_horizontal_cutoff\n",
    "\n",
    "\n",
    "def degree_day_core(\n",
    "    tmin_1d: np.ndarray,\n",
    "    tmax_1d: np.ndarray,\n",
    "    start_day: int,\n",
    "    stages: List[Dict],\n",
    "    generations: int = 3,\n",
    ") -> Dict[str, Union[Tuple[int, int, float], Tuple[str, int, float]]]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    - For complete generations: ('completed', days, accumulated_dd)\n",
    "    - For incomplete: ('stage_X_gen_Y', current_days, partial_dd)\n",
    "    \"\"\"\n",
    "    current_day = start_day\n",
    "    total_days = len(tmin_1d)\n",
    "\n",
    "    for gen in range(1, generations + 1):\n",
    "        stage_accumulator = 0.0\n",
    "\n",
    "        for stage_idx, stage in stages.items():\n",
    "            stage_dd = 0.0\n",
    "            days_in_stage = 0\n",
    "            while current_day < total_days:\n",
    "                # Calculate degree days for all remaining days\n",
    "                dd = single_sine_horizontal_cutoff(\n",
    "                    tmin_1d[current_day],\n",
    "                    tmax_1d[current_day],\n",
    "                    stage[\"LTT\"],\n",
    "                    stage[\"UTT\"],\n",
    "                )\n",
    "\n",
    "                stage_dd += dd\n",
    "                stage_accumulator += dd\n",
    "                days_in_stage += 1\n",
    "                current_day += 1\n",
    "\n",
    "                if stage_accumulator >= stage[\"dd_threshold\"]:\n",
    "                    break\n",
    "\n",
    "            if stage_accumulator < stage[\"dd_threshold\"]:\n",
    "                # Incomplete stage\n",
    "                days_elapsed = current_day - start_day\n",
    "                return days_elapsed, stage_dd, gen, stage_idx\n",
    "\n",
    "        # Generation completed\n",
    "        if gen == generations:\n",
    "            days_elapsed = current_day - start_day\n",
    "            return days_elapsed, stage_dd, gen, stage_idx\n",
    "\n",
    "    raise ValueError(\"generation accumulation failed\") #should not reach here\n",
    "\n",
    "\n",
    "# def spatial_wrapper(tmin_xr, tmax_xr, start_day, stages, generations=3):\n",
    "#     \"\"\"Returns xarray Dataset with completion status and metrics\"\"\"\n",
    "#     return xr.apply_ufunc(\n",
    "#         degree_day_core,\n",
    "#         tmin_xr,\n",
    "#         tmax_xr,\n",
    "#         input_core_dims=[[\"t\"], [\"t\"]],\n",
    "#         kwargs={\"start_day\": start_day, \"stages\": stages, \"generations\": generations},\n",
    "#         output_core_dims=[[], [], []],  # Three scalar outputs\n",
    "#         output_dtypes=[\"U20\", int, float],  # Status string, days, DD\n",
    "#         vectorize=True,\n",
    "#         dask=\"parallelized\",\n",
    "#         exclude_dims={\"t\"},  # We're reducing time dimension\n",
    "#     ).to_dataset(dim=\"output\", names=[\"status\", \"days\", \"accumulated_dd\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatial_wrapper(\n",
    "    tmin_xr: xr.DataArray,  # (time, lat, lon)\n",
    "    tmax_xr: xr.DataArray,  # (time, lat, lon)\n",
    "    start_day: int,\n",
    "    stages: List[Dict],\n",
    "    generations: int = 3,\n",
    ") -> xr.Dataset:\n",
    "    \"\"\"Simplified wrapper matching core outputs\"\"\"\n",
    "    days, dd, gen, stage = xr.apply_ufunc(\n",
    "        degree_day_core,\n",
    "        tmin_xr,\n",
    "        tmax_xr,\n",
    "        input_core_dims=[[\"t\"], [\"t\"]],\n",
    "        kwargs={\"start_day\": start_day, \"stages\": stages, \"generations\": generations},\n",
    "        output_core_dims=[[], [], [], []],\n",
    "        output_dtypes=[int, float, int, int],\n",
    "        vectorize=True,\n",
    "        dask=\"parallelized\",\n",
    "        exclude_dims={\"t\"},\n",
    "    )\n",
    "\n",
    "    return xr.Dataset(\n",
    "        {\n",
    "            \"days_elapsed\": days,\n",
    "            \"accumulated_dd\": dd,\n",
    "            \"current_gen\": gen,\n",
    "            \"current_stage\": stage,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'default': {'UTT': 999, 'LTT': 8.375, 'dd_threshold': 625}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "wrong number of outputs from pyfunc: expected 3, got 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m target_species = \u001b[33m\"\u001b[39m\u001b[33moff\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     15\u001b[39m stages = load_species_params(target_species, data_path)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m test = \u001b[43mspatial_wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtmin_xr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtmax_xr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_day\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerations\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m test_core = degree_day_core(\n\u001b[32m     21\u001b[39m     tmax_single_sample.values,\n\u001b[32m     22\u001b[39m     tmin_single_sample.values,\n\u001b[32m   (...)\u001b[39m\u001b[32m     25\u001b[39m     generations=\u001b[32m3\u001b[39m,\n\u001b[32m     26\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 66\u001b[39m, in \u001b[36mspatial_wrapper\u001b[39m\u001b[34m(tmin_xr, tmax_xr, start_day, stages, generations)\u001b[39m\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mspatial_wrapper\u001b[39m(tmin_xr, tmax_xr, start_day, stages, generations=\u001b[32m3\u001b[39m):\n\u001b[32m     65\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Returns xarray Dataset with completion status and metrics\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mxr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply_ufunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdegree_day_core\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtmin_xr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtmax_xr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_core_dims\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstart_day\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_day\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgenerations\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerations\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_core_dims\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Three scalar outputs\u001b[39;49;00m\n\u001b[32m     73\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_dtypes\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mU20\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Status string, days, DD\u001b[39;49;00m\n\u001b[32m     74\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvectorize\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdask\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallelized\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexclude_dims\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# We're reducing time dimension\u001b[39;49;00m\n\u001b[32m     77\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m.to_dataset(dim=\u001b[33m\"\u001b[39m\u001b[33moutput\u001b[39m\u001b[33m\"\u001b[39m, names=[\u001b[33m\"\u001b[39m\u001b[33mstatus\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mdays\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33maccumulated_dd\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CIPM/FruitFlyPheno/.venv/lib/python3.12/site-packages/xarray/computation/apply_ufunc.py:1268\u001b[39m, in \u001b[36mapply_ufunc\u001b[39m\u001b[34m(func, input_core_dims, output_core_dims, exclude_dims, vectorize, join, dataset_join, dataset_fill_value, keep_attrs, kwargs, dask, output_dtypes, output_sizes, meta, dask_gufunc_kwargs, on_missing_core_dim, *args)\u001b[39m\n\u001b[32m   1266\u001b[39m \u001b[38;5;66;03m# feed DataArray apply_variable_ufunc through apply_dataarray_vfunc\u001b[39;00m\n\u001b[32m   1267\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(a, DataArray) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m args):\n\u001b[32m-> \u001b[39m\u001b[32m1268\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mapply_dataarray_vfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1269\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvariables_vfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1270\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1271\u001b[39m \u001b[43m        \u001b[49m\u001b[43msignature\u001b[49m\u001b[43m=\u001b[49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1272\u001b[39m \u001b[43m        \u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1273\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexclude_dims\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexclude_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1274\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_attrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_attrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1275\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1276\u001b[39m \u001b[38;5;66;03m# feed Variables directly through apply_variable_ufunc\u001b[39;00m\n\u001b[32m   1277\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(a, Variable) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m args):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CIPM/FruitFlyPheno/.venv/lib/python3.12/site-packages/xarray/computation/apply_ufunc.py:310\u001b[39m, in \u001b[36mapply_dataarray_vfunc\u001b[39m\u001b[34m(func, signature, join, exclude_dims, keep_attrs, *args)\u001b[39m\n\u001b[32m    305\u001b[39m result_coords, result_indexes = build_output_coords_and_indexes(\n\u001b[32m    306\u001b[39m     args, signature, exclude_dims, combine_attrs=keep_attrs\n\u001b[32m    307\u001b[39m )\n\u001b[32m    309\u001b[39m data_vars = [\u001b[38;5;28mgetattr\u001b[39m(a, \u001b[33m\"\u001b[39m\u001b[33mvariable\u001b[39m\u001b[33m\"\u001b[39m, a) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m args]\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m result_var = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdata_vars\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    312\u001b[39m out: \u001b[38;5;28mtuple\u001b[39m[DataArray, ...] | DataArray\n\u001b[32m    313\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m signature.num_outputs > \u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CIPM/FruitFlyPheno/.venv/lib/python3.12/site-packages/xarray/computation/apply_ufunc.py:821\u001b[39m, in \u001b[36mapply_variable_ufunc\u001b[39m\u001b[34m(func, signature, exclude_dims, dask, output_dtypes, vectorize, keep_attrs, dask_gufunc_kwargs, *args)\u001b[39m\n\u001b[32m    816\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m vectorize:\n\u001b[32m    817\u001b[39m         func = _vectorize(\n\u001b[32m    818\u001b[39m             func, signature, output_dtypes=output_dtypes, exclude_dims=exclude_dims\n\u001b[32m    819\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m821\u001b[39m result_data = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m signature.num_outputs == \u001b[32m1\u001b[39m:\n\u001b[32m    824\u001b[39m     result_data = (result_data,)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CIPM/FruitFlyPheno/.venv/lib/python3.12/site-packages/numpy/lib/_function_base_impl.py:2522\u001b[39m, in \u001b[36mvectorize.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   2519\u001b[39m     \u001b[38;5;28mself\u001b[39m._init_stage_2(*args, **kwargs)\n\u001b[32m   2520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m2522\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_as_normal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CIPM/FruitFlyPheno/.venv/lib/python3.12/site-packages/numpy/lib/_function_base_impl.py:2515\u001b[39m, in \u001b[36mvectorize._call_as_normal\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   2512\u001b[39m     vargs = [args[_i] \u001b[38;5;28;01mfor\u001b[39;00m _i \u001b[38;5;129;01min\u001b[39;00m inds]\n\u001b[32m   2513\u001b[39m     vargs.extend([kwargs[_n] \u001b[38;5;28;01mfor\u001b[39;00m _n \u001b[38;5;129;01min\u001b[39;00m names])\n\u001b[32m-> \u001b[39m\u001b[32m2515\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_vectorize_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CIPM/FruitFlyPheno/.venv/lib/python3.12/site-packages/numpy/lib/_function_base_impl.py:2596\u001b[39m, in \u001b[36mvectorize._vectorize_call\u001b[39m\u001b[34m(self, func, args)\u001b[39m\n\u001b[32m   2594\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Vectorized call to `func` over positional `args`.\"\"\"\u001b[39;00m\n\u001b[32m   2595\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.signature \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2596\u001b[39m     res = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_vectorize_call_with_signature\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2597\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[32m   2598\u001b[39m     res = func()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CIPM/FruitFlyPheno/.venv/lib/python3.12/site-packages/numpy/lib/_function_base_impl.py:2641\u001b[39m, in \u001b[36mvectorize._vectorize_call_with_signature\u001b[39m\u001b[34m(self, func, args)\u001b[39m\n\u001b[32m   2638\u001b[39m n_results = \u001b[38;5;28mlen\u001b[39m(results) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m1\u001b[39m\n\u001b[32m   2640\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m nout != n_results:\n\u001b[32m-> \u001b[39m\u001b[32m2641\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2642\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mwrong number of outputs from pyfunc: expected \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m, got \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m   2643\u001b[39m         % (nout, n_results))\n\u001b[32m   2645\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m nout == \u001b[32m1\u001b[39m:\n\u001b[32m   2646\u001b[39m     results = (results,)\n",
      "\u001b[31mValueError\u001b[39m: wrong number of outputs from pyfunc: expected 3, got 1"
     ]
    }
   ],
   "source": [
    "# lets test the core first\n",
    "data_path = os.path.abspath(os.path.join(\"..\", \"data\"))\n",
    "cache_path = os.path.join(data_path, \"cache/pred_cache.pkl\")\n",
    "if cache_path and os.path.exists(cache_path):\n",
    "    with open(cache_path, \"rb\") as cache_file:\n",
    "        raw_PRISM = pickle.load(cache_file)\n",
    "\n",
    "tmin_xr = raw_PRISM[\"tmin\"]\n",
    "tmax_xr = raw_PRISM[\"tmax\"]\n",
    "tmax_single_sample = tmax_xr.isel(latitude=0, longitude=0)\n",
    "tmin_single_sample = tmin_xr.isel(latitude=0, longitude=0)\n",
    "\n",
    "start_day = 0\n",
    "target_species = \"off\"\n",
    "stages = load_species_params(target_species, data_path)\n",
    "\n",
    "test = spatial_wrapper(tmin_xr, tmax_xr, start_day, stages, generations=3)\n",
    "\n",
    "\n",
    "test_core = degree_day_core(\n",
    "    tmax_single_sample.values,\n",
    "    tmin_single_sample.values,\n",
    "    start_day,\n",
    "    stages,\n",
    "    generations=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'completed', 'days': 236, 'dd': np.float64(637.5014963150024)}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'completed', 'days': 236, 'dd': np.float64(637.5014963150024)}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mfly_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkeys\u001b[49m()\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'keys'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "from typing import List, Dict\n",
    "def get_remaining_requirements(\n",
    "    stages: List[Dict],\n",
    "    current_gen: int,\n",
    "    current_stage: int,\n",
    "    accumulated_dd: float,\n",
    "    generations_to_forecast: int,\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Calculates the exact remaining degree-day requirements when development is\n",
    "    partially through multiple generations.\n",
    "\n",
    "    Args:\n",
    "        stages: Complete stage definitions\n",
    "        current_gen: Current generation (1-based)\n",
    "        current_stage: Current stage index (0-based)\n",
    "        accumulated_dd: DD already accumulated in current stage\n",
    "        generations_to_forecast: How many future gens to model\n",
    "\n",
    "    Returns:\n",
    "        List of stage dictionaries with adjusted thresholds\n",
    "    \"\"\"\n",
    "    remaining_stages = []\n",
    "\n",
    "    # 1. Handle current generation\n",
    "    current_gen_stages = stages[current_stage:]\n",
    "    if current_gen_stages:\n",
    "        # Adjust first stage's threshold\n",
    "        adjusted_stage = current_gen_stages[0].copy()\n",
    "        adjusted_stage[\"dd_threshold\"] -= accumulated_dd\n",
    "        remaining_stages.append(adjusted_stage)\n",
    "        remaining_stages.extend(current_gen_stages[1:])\n",
    "\n",
    "    # 2. Handle complete future generations\n",
    "    gens_remaining = generations_to_forecast - (1 if current_gen_stages else 0)\n",
    "    for _ in range(gens_remaining):\n",
    "        remaining_stages.extend(stages)  # Full requirements for new generations\n",
    "\n",
    "    return remaining_stages\n",
    "\n",
    "\n",
    "def forecast_completion(\n",
    "    current_year_tmin: np.ndarray,\n",
    "    current_year_tmax: np.ndarray,\n",
    "    historical_tmin: xr.DataArray,  # shape (years=20, time=365)\n",
    "    historical_tmax: xr.DataArray,  # shape (years=20, time=365)\n",
    "    stages: List[Dict],\n",
    "    current_day: int,\n",
    "    accumulated_dd: float,\n",
    "    current_stage: int,\n",
    "    current_gen: int,\n",
    ") -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Forecasts completion using historical weather patterns.\n",
    "\n",
    "    Args:\n",
    "        current_year_tmin/tmax: 1D arrays up to current day\n",
    "        historical_tmin/tmax: 2D arrays (year, day) of past years\n",
    "        accumulated_dd: Degree-days already accumulated\n",
    "        current_stage/gen: Development progress\n",
    "    Returns:\n",
    "        xr.Dataset with forecast trajectories\n",
    "    \"\"\"\n",
    "    # 1. Extract remaining stage requirements\n",
    "    remaining_stages = stages[current_stage:]\n",
    "    remaining_stages[0][\"dd_threshold\"] -= accumulated_dd\n",
    "\n",
    "    # 2. Prepare container for forecasts\n",
    "    n_years = historical_tmin.sizes[\"year\"]\n",
    "    forecasts = []\n",
    "\n",
    "    # 3. Run simulation for each historical year\n",
    "    for year_idx in range(n_years):\n",
    "        # Get weather data starting from current day\n",
    "        future_tmin = historical_tmin.isel(year=year_idx, time=slice(current_day, None))\n",
    "        future_tmax = historical_tmax.isel(year=year_idx, time=slice(current_day, None))\n",
    "\n",
    "        # Combine current + historical weather\n",
    "        full_tmin = np.concatenate([current_year_tmin[:current_day], future_tmin])\n",
    "        full_tmax = np.concatenate([current_year_tmax[:current_day], future_tmax])\n",
    "\n",
    "        # Run simulation\n",
    "        result = degree_day_core(\n",
    "            full_tmin, full_tmax, 0, remaining_stages, generations=1\n",
    "        )  # Only need remaining development\n",
    "\n",
    "        forecasts.append(\n",
    "            {\n",
    "                \"year\": historical_tmin.year.values[year_idx],\n",
    "                \"completion_day\": result[\"days\"],\n",
    "                \"total_dd\": result[\"dd\"],\n",
    "                \"completed\": result[\"status\"] == \"completed\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # 4. Convert to xarray\n",
    "    return xr.Dataset.from_records(forecasts).set_index(year=\"year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast_with_historical(\n",
    "    current_year_data: xr.Dataset,  # tmin/tmax up to current day (time, lat, lon)\n",
    "    historical_data: xr.Dataset,  # 20 years of tmin/tmax (year, time, lat, lon)\n",
    "    incomplete_results: xr.Dataset,  # From original run (lat, lon)\n",
    "    stages: List[Dict],\n",
    "    generations: int = 3,\n",
    ") -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Runs completion forecasts using historical data patterns.\n",
    "\n",
    "    Returns:\n",
    "        xr.Dataset with dimensions (year, lat, lon) containing:\n",
    "        - completion_day: Total days needed (current + historical)\n",
    "        - generation: Final completed generation\n",
    "    \"\"\"\n",
    "\n",
    "    # Get current progress status\n",
    "    status_parts = incomplete_results[\"status\"].str.split(\"_\")\n",
    "    current_stage = status_parts.str[1].astype(int)\n",
    "    current_gen = status_parts.str[3].astype(int)\n",
    "\n",
    "    # Prepare output arrays\n",
    "    n_years = len(historical_data.year)\n",
    "    shape = (n_years, len(incomplete_results.lat), len(incomplete_results.lon))\n",
    "    completion_days = np.full(shape, np.nan, dtype=int)\n",
    "    final_generations = np.full(shape, np.nan, dtype=int)\n",
    "\n",
    "    # Process each historical year\n",
    "    for i, year in enumerate(historical_data.year.values):\n",
    "        # Combine current + historical weather\n",
    "        combined_tmin = xr.concat(\n",
    "            [current_year_data[\"tmin\"], historical_data[\"tmin\"].sel(year=year)],\n",
    "            dim=\"time\",\n",
    "        )\n",
    "\n",
    "        combined_tmax = xr.concat(\n",
    "            [current_year_data[\"tmax\"], historical_data[\"tmax\"].sel(year=year)],\n",
    "            dim=\"time\",\n",
    "        )\n",
    "\n",
    "        # Run full simulation starting from day 0\n",
    "        results = xr.apply_ufunc(\n",
    "            your_original_core_function,  # Use your exact existing function\n",
    "            combined_tmin,\n",
    "            combined_tmax,\n",
    "            input_core_dims=[[\"time\"], [\"time\"]],\n",
    "            kwargs={\"stages\": stages, \"generations\": generations},\n",
    "            vectorize=True,\n",
    "            dask=\"parallelized\",\n",
    "            output_dtypes=[object],  # For dictionary results\n",
    "        )\n",
    "\n",
    "        # Extract completion metrics\n",
    "        for lat in incomplete_results.lat.values:\n",
    "            for lon in incomplete_results.lon.values:\n",
    "                cell_result = results.sel(lat=lat, lon=lon).item()\n",
    "                completion_days[i, lat, lon] = cell_result[f\"F{current_gen}\"][-1][0]\n",
    "                final_generations[i, lat, lon] = (\n",
    "                    current_gen if \"completed\" in cell_result else current_gen - 1\n",
    "                )\n",
    "\n",
    "    # Package results\n",
    "    return xr.Dataset(\n",
    "        {\n",
    "            \"completion_day\": ((\"year\", \"lat\", \"lon\"), completion_days),\n",
    "            \"final_generation\": ((\"year\", \"lat\", \"lon\"), final_generations),\n",
    "        },\n",
    "        coords={\n",
    "            \"year\": historical_data.year,\n",
    "            \"lat\": incomplete_results.lat,\n",
    "            \"lon\": incomplete_results.lon,\n",
    "        },\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original simulation\n",
    "results = fflies_model_2_multistage(current_year_data, ...)\n",
    "\n",
    "# Get incomplete cells\n",
    "incomplete = results.where(results.status != \"completed\", drop=True)\n",
    "\n",
    "# Run forecasts\n",
    "forecasts = forecast_with_historical(\n",
    "    current_year_data, historical_weather, incomplete, stages\n",
    ")\n",
    "\n",
    "# Calculate statistics\n",
    "mean_days = forecasts.completion_day.mean(dim=\"year\")\n",
    "prob_completed = (forecasts.final_generation >= 3).mean(dim=\"year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_plot_DD(\n",
    "    start_dates,\n",
    "    coordinates,\n",
    "    target_species,\n",
    "    historical_data_buffer=400,\n",
    "    cache_path=os.path.join(data_path, \"cache/pred_cache.pkl\"),\n",
    "    context_map=False,\n",
    "    save_output=False,\n",
    "    all_historical=None,\n",
    "    force_prediction=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates a plot of completion dates for a given species at specified coordinates.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    start_dates : list of datetime\n",
    "        List of start dates corresponding to each coordinate.\n",
    "    coordinates : list of tuples (lat, lon)\n",
    "        List of latitude and longitude pairs.\n",
    "    target_species : str\n",
    "        Name of the species to model.\n",
    "    days_of_data : int\n",
    "        Number of days of data to fetch - usually 180-200 is sufficient\n",
    "    historical_data_buffer : int\n",
    "        Number of days to buffer historical data for model run.\n",
    "    cache_path : str\n",
    "        Path to cache file for storing fetched data.\n",
    "    context_map : bool\n",
    "        If True, generates a context map for a single point.\n",
    "    save_output : bool\n",
    "        If True, saves the output data to a tiff file.\n",
    "    Returns:\n",
    "    -------\n",
    "    matplotlib.figure.Figure\n",
    "        A plot displaying degree-day completion for the given coordinates.\n",
    "\n",
    "    Raises:\n",
    "    ------\n",
    "    ValueError:\n",
    "        If the number of coordinates and dates do not match.\n",
    "        If no coordinates are provided.\n",
    "        If the start date is too early or the end date is in the future.\n",
    "    \"\"\"\n",
    "    ###################\n",
    "    ##Input Validation#\n",
    "    ###################\n",
    "    all_historical = validate_inputs(start_dates, coordinates, historical_data_buffer)\n",
    "\n",
    "    # a bounding box if we have multiple points, a context map if we have a single point\n",
    "    # loaded from .json file\n",
    "    fly_params = load_species_params(target_species, data_path)\n",
    "\n",
    "    # setup Modelvariables\n",
    "    first_date = min(start_dates)\n",
    "    last_date = max(start_dates) + timedelta(days=historical_data_buffer)\n",
    "    n_days_data = (last_date - first_date).days\n",
    "\n",
    "    ##########\n",
    "    ##Model##\n",
    "    ##########\n",
    "    if all_historical:\n",
    "        try:\n",
    "            return all_historical_model_run(\n",
    "                coordinates,\n",
    "                start_dates,\n",
    "                fly_params,\n",
    "                n_days_data,\n",
    "                cache_path,\n",
    "                context_map,\n",
    "            )\n",
    "        except PredictionNeededError: #triggers if we assume we have all historical data but we actually need to predict\n",
    "            all_historical = False\n",
    "    if force_prediction or not all_historical:\n",
    "        return prediction_model_run(\n",
    "            coordinates,\n",
    "            start_dates,\n",
    "            fly_params,\n",
    "            n_days_data,\n",
    "            cache_path,\n",
    "            produce_plot=False,\n",
    "        )\n",
    "        # predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = run_plot_DD(\n",
    "    [pd.to_datetime(\"2025-01-01\")],\n",
    "    [(34.63115, -117.338321)],\n",
    "    \"Mexfly\",\n",
    "    context_map=False,\n",
    "    cache_path=os.path.join(data_path, \"cache/pred_cache.pkl\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fflies_output_class(finish_date_list=[237, 230, 230, 245, 231], figure=<Figure size 640x480 with 0 Axes>, value=None, array=<xarray.DataArray ()> Size: 8B\n",
      "array(nan))\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'strptime'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mrun_plot_DD\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_datetime\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m2002-01-01\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m34.63115\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m117.338321\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mMexfly\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcache/cache.pkl\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 64\u001b[39m, in \u001b[36mrun_plot_DD\u001b[39m\u001b[34m(start_dates, coordinates, target_species, historical_data_buffer, cache_path, context_map, save_output, all_historical, force_prediction)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m all_historical:\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mall_historical_model_run\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcoordinates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstart_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfly_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_days_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcache_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcontext_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     72\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m PredictionNeededError:\n\u001b[32m     73\u001b[39m         all_historical = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CIPM/FruitFlyPheno/utils/simulations.py:265\u001b[39m, in \u001b[36mall_historical_model_run\u001b[39m\u001b[34m(coordinates, start_dates, fly_params, historical_data_buffer, cache_path, context_map, retry_count)\u001b[39m\n\u001b[32m    262\u001b[39m         raw_PRISM = pickle.load(cache_file)\n\u001b[32m    263\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    264\u001b[39m     \u001b[38;5;66;03m# Fetch historical data\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m265\u001b[39m     raw_PRISM = \u001b[43mfetch_ncss_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstart_date\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfirst_date_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_days\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_days_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcoordinates_bbox\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    269\u001b[39m     \u001b[38;5;66;03m# Save fetched data to cache\u001b[39;00m\n\u001b[32m    270\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m cache_path:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CIPM/FruitFlyPheno/utils/inputs.py:62\u001b[39m, in \u001b[36mfetch_ncss_data\u001b[39m\u001b[34m(start_date, n_days, bbox, variables, point, base_url)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     49\u001b[39m \u001b[33;03mFetch data from a THREDDS server using NCSS and collect it into an xarray Dataset.\u001b[39;00m\n\u001b[32m     50\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     59\u001b[39m \u001b[33;03m- xarray.Dataset containing the requested data\u001b[39;00m\n\u001b[32m     60\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m# Convert start_date to datetime\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m start_date = \u001b[43mstart_date\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstrptime\u001b[49m(start_date, \u001b[33m\"\u001b[39m\u001b[33m%\u001b[39m\u001b[33mY-\u001b[39m\u001b[33m%\u001b[39m\u001b[33mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m# Calculate end_date\u001b[39;00m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_days \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mAttributeError\u001b[39m: 'str' object has no attribute 'strptime'"
     ]
    }
   ],
   "source": [
    "run_plot_DD(\n",
    "    [pd.to_datetime(\"2002-01-01\")],\n",
    "    [(34.63115, -117.338321)],\n",
    "    \"Mexfly\",\n",
    "    context_map=True,\n",
    "    cache_path=os.path.join(data_path, \"cache/cache[deleted].pkl\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'run_plot_DD' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrun_plot_DD\u001b[49m(\n\u001b[1;32m      2\u001b[0m     [pd\u001b[38;5;241m.\u001b[39mto_datetime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2002-01-01\u001b[39m\u001b[38;5;124m\"\u001b[39m)],\n\u001b[1;32m      3\u001b[0m     [(\u001b[38;5;241m34.63115\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m117.338321\u001b[39m)],\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMexfly\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'run_plot_DD' is not defined"
     ]
    }
   ],
   "source": [
    "run_plot_DD(\n",
    "    [pd.to_datetime(\"2002-01-01\")],\n",
    "    [(34.63115, -117.338321)],\n",
    "    \"Mexfly\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run_plot_DD(\n",
    "    [pd.to_datetime(\"2002-01-01\")],\n",
    "    [(34.63115, -117.338321)],\n",
    "    \"Mexfly\",\n",
    "    context_map=False,\n",
    ")\n",
    "run_plot_DD(\n",
    "    [pd.to_datetime(\"2002-01-01\")],\n",
    "    [(34.63115, -117.338321)],\n",
    "    \"Mexfly\",\n",
    "    context_map=True,\n",
    ")\n",
    "### Why is map not plotting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nrun_plot_DD(\\n    [pd.to_datetime(\"2002-01-01\")],\\n    [(34.63115, -117.338321)],\\n    \"Mexfly\",\\n\\n\\nrun_plot_DD(\\n    [pd.to_datetime(\"2002-01-01\"), pd.to_datetime(\"2002-05-01\")],\\n    [(34.63115, -117.338321), (34.68115, -117.336321)],\\n    \"Mexfly\",\\n)\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test statements\n",
    "\"\"\"\n",
    "run_plot_DD(\n",
    "    [pd.to_datetime(\"2002-01-01\")],\n",
    "    [(34.63115, -117.338321)],\n",
    "    \"Mexfly\",\n",
    "\n",
    "\n",
    "run_plot_DD(\n",
    "    [pd.to_datetime(\"2002-01-01\"), pd.to_datetime(\"2002-05-01\")],\n",
    "    [(34.63115, -117.338321), (34.68115, -117.336321)],\n",
    "    \"Mexfly\",\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NCSS data saved to cache/pred_cache.pkl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "# Parameters\n",
    "start_date = \"2020-01-01\"\n",
    "bbox = (-117.63832099999999, -117.038321, 34.33115, 34.931149999999995)\n",
    "cache_path = \"cache/pred_cache.pkl\"\n",
    "\n",
    "# Ensure the cache directory exists\n",
    "os.makedirs(os.path.dirname(cache_path), exist_ok=True)\n",
    "\n",
    "# Fetch the data\n",
    "ncss_data = fetch_ncss_data(start_date=start_date, bbox=bbox)\n",
    "\n",
    "# Save the data to the cache\n",
    "with open(cache_path, \"wb\") as cache_file:\n",
    "    pickle.dump(ncss_data, cache_file)\n",
    "\n",
    "print(f\"NCSS data saved to {cache_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
