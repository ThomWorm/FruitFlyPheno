{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pydap.client import open_url\n",
    "import xarray as xr\n",
    "import datetime\n",
    "from datetime import timedelta, date\n",
    "import netCDF4 as nc\n",
    "from netCDF4 import Dataset\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "from utils.degree_day_equations import *\n",
    "from utils.net_cdf_functions import *\n",
    "from utils.processing_functions import *\n",
    "#from utils.visualization_functions import *\n",
    "\n",
    "import pandas as pd\n",
    "#from visualization_functions import *\n",
    "import numpy as np\n",
    "data_path =  \"../data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fetch_ncss_data( start_date, n_days=None, bbox=None, variables=['tmin', 'tmax'], point = None, base_url = \"https://thredds.climate.ncsu.edu/thredds/ncss/grid/prism/daily/combo\",):\n",
    "    \"\"\"\n",
    "    Fetch data from a THREDDS server using NCSS and collect it into an xarray Dataset.\n",
    "    Parameters:\n",
    "    - base_url: str, base URL of the THREDDS server\n",
    "    - start_date: str, start date in the format 'YYYY-MM-DD'\n",
    "    - n_days: int or None, number of days to fetch data for. If None, fetch all data to the present.\n",
    "    - bbox: tuple or None, bounding box in the format (lon_min, lon_max, lat_min, lat_max)\n",
    "    - variables: list or None, list of variables to fetch\n",
    "    \n",
    "    Returns:\n",
    "    - xarray.Dataset containing the requested data\n",
    "    \"\"\"\n",
    "    # Convert start_date to datetime\n",
    "    start_date = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "    \n",
    "    # Calculate end_date\n",
    "    if n_days is None:\n",
    "        end_date = datetime.now() - timedelta(days=2)\n",
    "    else:\n",
    "        end_date = start_date + timedelta(days=n_days)\n",
    "    \n",
    "    # Generate list of dates\n",
    "    dates = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "    \n",
    "    # Initialize an empty list to store NCSS URLs\n",
    "    ncss_urls = []\n",
    "    \n",
    "    # Loop through each date and construct the NCSS URL\n",
    "    for date in dates:\n",
    "        date_str = date.strftime('%Y-%m-%dT00:00:00Z')\n",
    "        year = date.strftime('%Y')\n",
    "        url = f\"{base_url}/{year}/PRISM_combo_{date.strftime('%Y%m%d')}.nc\"\n",
    "        \n",
    "        # Construct the NCSS URL\n",
    "        var_params = \"&\".join([f\"var={var}\" for var in variables])\n",
    "        if bbox:\n",
    "            ncss_url = (\n",
    "            f\"{url}?{var_params}\"\n",
    "            f\"&north={bbox[3]}&west={bbox[0]}&east={bbox[1]}&south={bbox[2]}\"\n",
    "            f\"&horizStride=1&time_start={date_str}&time_end={date_str}&accept=netcdf4ext&addLatLon=true\"\n",
    "            )\n",
    "        elif point:\n",
    "            ncss_url = (\n",
    "                f\"{url}?{var_params}\"\n",
    "                f\"&north={point[1]}&west={point[0]}&east={point[0]}&south={point[1]}\"\n",
    "                f\"&horizStride=1&time_start={date_str}&time_end={date_str}&accept=netcdf4ext&addLatLon=true\"\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"Either bbox or point must be provided.\")\n",
    "            # Append the NCSS URL to the list\n",
    "\n",
    "        ncss_urls.append(ncss_url)\n",
    "    \n",
    "    # Initialize an empty list to store datasets\n",
    "    datasets = [None] * len(ncss_urls)\n",
    "    \n",
    "    # Use ThreadPoolExecutor to fetch data in parallel\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        future_to_index = {executor.submit(fetch_single_day_ncss, url): i for i, url in enumerate(ncss_urls)}\n",
    "        for future in as_completed(future_to_index):\n",
    "            index = future_to_index[future]\n",
    "            try:\n",
    "                ds = future.result()\n",
    "                datasets[index] = ds\n",
    "            except Exception as e:\n",
    "\n",
    "                try:\n",
    "                    #wait 5 seconds\n",
    "                    time.sleep(3)\n",
    "                    ds = future.result()\n",
    "                    datasets.append(ds)\n",
    "                except:\n",
    "                   print(e)\n",
    "                   print(f\"Error fetching data for URL {ncss_urls[index]}: {e}\")\n",
    "    \n",
    "    # Combine all datasets into a single xarray Dataset\n",
    "    combined_ds = xr.concat(datasets, dim='t', join = 'override')\n",
    "    \n",
    "    return combined_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thomas.Worm\\AppData\\Local\\Temp\\ipykernel_63228\\2238321212.py:4: UserWarning: URL https://thredds.climate.ncsu.edu/thredds/catalog/prism/daily/combo/1982/catalog.html returned HTML. Changing to: https://thredds.climate.ncsu.edu/thredds/catalog/prism/daily/combo/1982/catalog.xml\n",
      "  test_dat = TDSCatalog('https://thredds.climate.ncsu.edu/thredds/catalog/prism/daily/combo/1982/catalog.html').datasets\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DatasetCollection' object has no attribute 'subset'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mxarray\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mxr\u001b[39;00m\n\u001b[0;32m      4\u001b[0m test_dat \u001b[38;5;241m=\u001b[39m TDSCatalog(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://thredds.climate.ncsu.edu/thredds/catalog/prism/daily/combo/1982/catalog.html\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mdatasets\n\u001b[1;32m----> 5\u001b[0m ncss \u001b[38;5;241m=\u001b[39m \u001b[43mtest_dat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubset\u001b[49m()\n\u001b[0;32m      6\u001b[0m query \u001b[38;5;241m=\u001b[39m ncss\u001b[38;5;241m.\u001b[39mquery()    \n\u001b[0;32m      7\u001b[0m query\u001b[38;5;241m.\u001b[39mlonlat_point(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m105\u001b[39m, \u001b[38;5;241m40\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DatasetCollection' object has no attribute 'subset'"
     ]
    }
   ],
   "source": [
    "from siphon.catalog import TDSCatalog\n",
    "from siphon.http_util import session_manager\n",
    "import xarray as xr\n",
    "test_dat = TDSCatalog('https://thredds.climate.ncsu.edu/thredds/catalog/prism/daily/combo/1982/catalog.html').datasets\n",
    "ncss = test_dat.subset()\n",
    "query = ncss.query()    \n",
    "query.lonlat_point(-105, 40)\n",
    "query.variables('tmax', 'tmin')\n",
    "query.accept('netcdf4')\n",
    "data = ncss.get_data(query)\n",
    "#ds = xr.open_dataset(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'netCDF4._netCDF4.Dataset'>\n",
       "root group (NETCDF4_CLASSIC data model, file format HDF5):\n",
       "    Conventions: CF-1.9\n",
       "    history: Written by CFPointWriter\n",
       "    title: Extracted data from TDS Feature Collection null\n",
       "    geospatial_lat_min: 39.9995\n",
       "    geospatial_lat_max: 40.0005\n",
       "    geospatial_lon_min: -105.00049999955267\n",
       "    geospatial_lon_max: -104.99949999955267\n",
       "    featureType: timeSeries\n",
       "    DSG_representation: Timeseries of station data in the indexed ragged array representation, H.2.5\n",
       "    time_coverage_start: 1982-01-01T00:00:00Z\n",
       "    time_coverage_end: 1982-01-01T00:00:00Z\n",
       "    dimensions(sizes): obs(1), station(1), station_description_strlen(38), station_id_strlen(38)\n",
       "    variables(dimensions): float64 latitude(station), float64 longitude(station), float64 stationAltitude(station), |S1 station_id(station, station_id_strlen), |S1 station_description(station, station_description_strlen), float64 tmin(obs), float64 tmax(obs), float64 time(obs), int32 stationIndex(obs)\n",
       "    groups: "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fruitflypheno",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
